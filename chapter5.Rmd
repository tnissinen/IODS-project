# Chapter 5: Dimensionality reduction techniques

### Loading and checking the structure of the data

```{r readdata_human,echo=TRUE,results='hide',message=FALSE,warning=FALSE}

# access libraries
library(MASS)
library(dplyr)
library(tidyr)
library(corrplot)
library(ggplot2)
library(GGally)
library(FactoMineR)

```

```{r}
# load the human data
human <- read.csv("~/IODS-project/data/human.csv")

# explore the dataset
str(human)
summary(human)

```

The dataset combines several indicators from most countries in the world. It has 155 observations with 9 variables.


### Graphical overview of the data

Plot the distributions and correlations of the variables:

```{r fig_human2, fig.path="figures/", fig.dim=c(10,10), results='hide', message=FALSE, warning=FALSE}

# for visualizations, take the data without the name column
human_ <- human[-1]

# visualize the 'human_' variables
ggpairs(human_)

# compute the correlation matrix and visualize it with corrplot
cor(human_) %>% corrplot()

```


From the distribution of GNI we see that the majority of countries are quite poor. GNI seems to be positively correlated with Life expectancy at birth and Expected years of schooling. It's negatively correlated with Maternal mortality ratio and Adolescent birth rate. The are somewhat expected observations.

### Principal component analysis

Non-standardized
```{r}

# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_)

# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.6, 1.2), col = c("grey40", "deeppink2"))


```

PCA is sensitive to the relative scaling of the original features. It assumes variables with larger variance to be more important. For this reason, the GNI value is basically the only one popping out from the plot.


Standardized
```{r}

# standardize the variables
human_std <- scale(human_)

# create and print out a summary of pca_human
pca_human_std <- prcomp(human_std)
s <- summary(pca_human_std)

# rounded percentanges of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)

# print out the percentages of variance
pca_pr

# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")

# draw a biplot
biplot(pca_human_std, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])

```

With standardized data the PCA works a lot better. We can see that PC1 explains over 50% of the variance in the data. It consists of most of the variables in the data. In short it seems to show the economical standard of living in the country.  The PC2 contributes to about 16% of the variance and represents variables Percetange of female representatives in parliament and Proportion of females in the labour force. The PC2 can be seen as the position of female in the country.




### Analysing the tea dataset


```{r}

data(tea)

# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")

# select the 'keep_columns' to create a new dataset
tea_time <- dplyr::select(tea, one_of(keep_columns))

# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)

# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) + facet_wrap("key", scales = "free")

```

### Multiple Correspondence Analysis on tea

Multiple Correspondence Analysis (MCA) is a method to analyze qualitative data and it is an extension of Correspondence analysis (CA). MCA can be used to detect patterns or structure in the data as well as in dimension reduction.


```{r}

# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)

# summary of the model
summary(mca)

# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")

```


We can see that there are two main types of tea consumers. The main stream consumers that buy their tea from the super market and then the tea hipsters that but unpacked tea from tea shops.
